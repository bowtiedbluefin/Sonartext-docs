---
title: 'Create Transcription'
openapi: 'POST /v1/audio/transcriptions'
---

OpenAI-compatible transcription endpoint with multiple file source options. Convert audio and video files to text with advanced features like speaker diarization, multiple output formats, and flexible input methods.

## File Source Options

<Tabs>
<Tab title="Direct Upload">
**Most common method**: Upload file directly in the request body.

- Best for files under 150MB
- Immediate processing
- Simple implementation

```bash
curl -X POST 'https://api.sonartext.com/v1/audio/transcriptions' \
  -H 'Authorization: Bearer stx_live_your_api_key' \
  -F 'file=@audio.wav' \
  -F 'model=whisper-1'
```
</Tab>

<Tab title="R2 Reference">
**For re-transcription**: Use `r2_key` from previous multipart upload.

- Reuse uploaded files with different parameters
- No re-upload needed
- Cost-effective for multiple transcriptions

```bash
curl -X POST 'https://api.sonartext.com/v1/audio/transcriptions' \
  -H 'Authorization: Bearer stx_live_your_api_key' \
  -H 'Content-Type: application/json' \
  -d '{
    "r2_bucket": "sonartext-uploads",
    "r2_key": "uploads/user123/1234567890-audio.wav",
    "model": "whisper-1"
  }'
```
</Tab>

<Tab title="S3 Presigned URL">
**For integrations**: Provide presigned URL for external storage.

- Integrate with your own S3/R2/GCS buckets
- No file transfer through our API
- Great for large files in external storage

```bash
curl -X POST 'https://api.sonartext.com/v1/audio/transcriptions' \
  -H 'Authorization: Bearer stx_live_your_api_key' \
  -H 'Content-Type: application/json' \
  -d '{
    "download_url": "https://mybucket.s3.amazonaws.com/file.wav?X-Amz-Signature=...",
    "model": "whisper-1"
  }'
```
</Tab>
</Tabs>

## Request Parameters

### File Source (Choose One)

<ParamField body="file" type="file">
  Audio or video file to transcribe (for direct upload method).
  
  **Supported formats**: MP3, WAV, FLAC, AAC, MP4, MOV, AVI, and many more
  **Max size**: 2GB for direct uploads
</ParamField>

<ParamField body="r2_bucket" type="string">
  R2 bucket name (use "sonartext-uploads" unless you have your own R2).
  
  **Required with**: `r2_key`
  **Example**: `"sonartext-uploads"`
</ParamField>

<ParamField body="r2_key" type="string">
  R2 object key from `/v1/uploads/initiate` response. Use this to re-transcribe files.
  
  **Required with**: `r2_bucket`
  **Example**: `"uploads/user123/1234567890-audio.wav"`
</ParamField>

<ParamField body="download_url" type="string">
  Presigned URL to download file from S3-compatible storage.
  
  **Use case**: Integration with external storage systems
  **Example**: `"https://mybucket.s3.amazonaws.com/file.wav?X-Amz-Signature=..."`
</ParamField>

### Transcription Parameters

<ParamField body="model" type="string" default="whisper-1">
  Model to use for transcription. Currently ignored - always uses large-v3 for best quality.
  
  **Example**: `"whisper-1"`
</ParamField>

<ParamField body="language" type="string">
  Language code (ISO 639-1) for the audio. Auto-detected if not specified.
  
  **Common codes**: `en` (English), `es` (Spanish), `fr` (French), `de` (German), `zh` (Chinese)
</ParamField>

<ParamField body="response_format" type="string" default="json">
  Output format for transcription results.
  
  **Options**:
  - `json`: Basic JSON with text and segments
  - `verbose_json`: Detailed JSON with timestamps and metadata
  - `text`: Plain text only
  - `srt`: SubRip subtitle format
  - `vtt`: WebVTT subtitle format
</ParamField>

<ParamField body="timestamp_granularities" type="array" default="[segment]">
  Level of timestamp detail to include.
  
  **Options**:
  - `segment`: Timestamps for each segment/sentence
  - `word`: Timestamps for each individual word
  
  **Example**: `["segment", "word"]` for both levels
</ParamField>

<ParamField body="enable_diarization" type="boolean" default="false">
  Enable speaker identification to distinguish between different speakers.
  
  <Note>
    When enabled, each segment includes a speaker ID. Use `min_speakers` and `max_speakers` to guide the process.
  </Note>
</ParamField>

<ParamField body="min_speakers" type="integer">
  Minimum number of speakers expected (1-10). Only used with diarization.
  
  **Example**: `2` for a conversation between two people
</ParamField>

<ParamField body="max_speakers" type="integer">
  Maximum number of speakers expected (1-10). Only used with diarization.
  
  **Example**: `5` for a meeting with up to 5 participants
</ParamField>

<ParamField body="output_content" type="string" default="both">
  What to include in the transcription output.
  
  **Options**:
  - `both`: Include both text and timestamps
  - `text_only`: Only transcribed text
  - `timestamps_only`: Only timestamp information  
  - `metadata_only`: Only metadata (language, duration, etc.)
</ParamField>

## Response Format

The response format depends on the `response_format` parameter:

### JSON Response (verbose_json)

<ResponseField name="text" type="string" required>
  Complete transcription text.
</ResponseField>

<ResponseField name="segments" type="array" required>
  Array of transcription segments with timestamps.
  
  <Expandable title="Segment Object">
    <ResponseField name="id" type="integer">
      Segment identifier.
    </ResponseField>
    <ResponseField name="start" type="number">
      Start time in seconds.
    </ResponseField>
    <ResponseField name="end" type="number">
      End time in seconds.
    </ResponseField>
    <ResponseField name="text" type="string">
      Segment text content.
    </ResponseField>
    <ResponseField name="speaker" type="string">
      Speaker ID (if diarization enabled).
    </ResponseField>
    <ResponseField name="confidence" type="number">
      Confidence score (0.0 to 1.0).
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="words" type="array">
  Word-level timestamps (if word granularity requested).
  
  <Expandable title="Word Object">
    <ResponseField name="word" type="string">
      Individual word.
    </ResponseField>
    <ResponseField name="start" type="number">
      Word start time in seconds.
    </ResponseField>
    <ResponseField name="end" type="number">
      Word end time in seconds.
    </ResponseField>
    <ResponseField name="confidence" type="number">
      Word confidence score.
    </ResponseField>
    <ResponseField name="speaker" type="string">
      Speaker ID (if diarization enabled).
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="language" type="string" required>
  Detected language code.
</ResponseField>

<ResponseField name="duration" type="number" required>
  Total audio duration in seconds.
</ResponseField>

<ResponseField name="speakers" type="array">
  Speaker information (if diarization enabled).
  
  <Expandable title="Speaker Object">
    <ResponseField name="id" type="string">
      Speaker identifier.
    </ResponseField>
    <ResponseField name="label" type="string">
      Speaker label (e.g., "Speaker 1").
    </ResponseField>
    <ResponseField name="segments" type="array">
      Array of segment IDs for this speaker.
    </ResponseField>
  </Expandable>
</ResponseField>

## Example Usage

<CodeGroup>
```javascript Direct Upload
async function transcribeFile(file, options = {}) {
  const formData = new FormData();
  formData.append('file', file);
  formData.append('model', options.model || 'whisper-1');
  
  if (options.language) {
    formData.append('language', options.language);
  }
  
  if (options.diarization) {
    formData.append('enable_diarization', 'true');
    if (options.minSpeakers) formData.append('min_speakers', options.minSpeakers);
    if (options.maxSpeakers) formData.append('max_speakers', options.maxSpeakers);
  }
  
  formData.append('response_format', options.format || 'verbose_json');
  formData.append('timestamp_granularities', JSON.stringify(options.granularity || ['segment']));

  const response = await fetch('/v1/audio/transcriptions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`
    },
    body: formData
  });

  if (!response.ok) {
    throw new Error(`Transcription failed: ${response.statusText}`);
  }

  return response.json();
}

// Usage
const result = await transcribeFile(audioFile, {
  language: 'en',
  format: 'verbose_json',
  diarization: true,
  minSpeakers: 2,
  maxSpeakers: 4,
  granularity: ['segment', 'word']
});
```

```python R2 Re-transcription
import requests

def retranscribe_from_r2(r2_key, api_key, **options):
    url = 'https://api.sonartext.com/v1/audio/transcriptions'
    
    payload = {
        'r2_bucket': 'sonartext-uploads',
        'r2_key': r2_key,
        'model': options.get('model', 'whisper-1'),
        'response_format': options.get('response_format', 'verbose_json'),
        'timestamp_granularities': options.get('timestamp_granularities', ['segment'])
    }
    
    if 'language' in options:
        payload['language'] = options['language']
    
    if options.get('enable_diarization'):
        payload['enable_diarization'] = True
        if 'min_speakers' in options:
            payload['min_speakers'] = options['min_speakers']
        if 'max_speakers' in options:
            payload['max_speakers'] = options['max_speakers']
    
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    response = requests.post(url, json=payload, headers=headers)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"Transcription failed: {response.text}")

# Usage - re-transcribe with different settings
result = retranscribe_from_r2(
    r2_key='uploads/user123/1234567890-audio.wav',
    api_key='stx_live_your_api_key',
    response_format='srt',  # Different format
    enable_diarization=False  # Different settings
)
```

```curl S3 Presigned URL
curl -X POST 'https://api.sonartext.com/v1/audio/transcriptions' \
  -H 'Authorization: Bearer stx_live_your_api_key' \
  -H 'Content-Type: application/json' \
  -d '{
    "download_url": "https://mybucket.s3.amazonaws.com/meeting.wav?X-Amz-Algorithm=AWS4-HMAC-SHA256&...",
    "model": "whisper-1",
    "language": "en",
    "response_format": "verbose_json",
    "enable_diarization": true,
    "min_speakers": 2,
    "max_speakers": 5,
    "timestamp_granularities": ["segment", "word"]
  }'
```
</CodeGroup>

## Response Examples

<ResponseExample>
```json Verbose JSON (with Diarization)
{
  "text": "Welcome to today's meeting. Let's start with the quarterly review. Thank you, I have the numbers ready.",
  "segments": [
    {
      "id": 0,
      "start": 0.5,
      "end": 3.2,
      "text": "Welcome to today's meeting.",
      "speaker": "Speaker_1",
      "confidence": 0.95
    },
    {
      "id": 1,
      "start": 3.5,
      "end": 6.8,
      "text": "Let's start with the quarterly review.",
      "speaker": "Speaker_1", 
      "confidence": 0.92
    },
    {
      "id": 2,
      "start": 7.1,
      "end": 10.3,
      "text": "Thank you, I have the numbers ready.",
      "speaker": "Speaker_2",
      "confidence": 0.89
    }
  ],
  "words": [
    {
      "word": "Welcome",
      "start": 0.5,
      "end": 0.9,
      "confidence": 0.98,
      "speaker": "Speaker_1"
    },
    {
      "word": "to",
      "start": 0.9,
      "end": 1.1,
      "confidence": 0.99,
      "speaker": "Speaker_1"
    }
    // ... more words
  ],
  "language": "en",
  "duration": 10.3,
  "speakers": [
    {
      "id": "Speaker_1",
      "label": "Speaker 1",
      "segments": [0, 1]
    },
    {
      "id": "Speaker_2", 
      "label": "Speaker 2",
      "segments": [2]
    }
  ]
}
```
</ResponseExample>

<ResponseExample>
```srt SRT Format
1
00:00:00,500 --> 00:00:03,200
<v Speaker_1>Welcome to today's meeting.

2
00:00:03,500 --> 00:00:06,800
<v Speaker_1>Let's start with the quarterly review.

3
00:00:07,100 --> 00:00:10,300
<v Speaker_2>Thank you, I have the numbers ready.
```
</ResponseExample>

<ResponseExample>
```vtt WebVTT Format
WEBVTT

NOTE
Generated by Sonartext API

00:00:00.500 --> 00:00:03.200
<v Speaker_1>Welcome to today's meeting.

00:00:03.500 --> 00:00:06.800
<v Speaker_1>Let's start with the quarterly review.

00:00:07.100 --> 00:00:10.300
<v Speaker_2>Thank you, I have the numbers ready.
```
</ResponseExample>

## Re-transcription Workflow

<Steps>
<Step title="Initial Upload">
  Upload a large file using multipart upload and get the `r2Key`.
  
  ```javascript
  const uploadResult = await completeUpload(uploadId, parts);
  const r2Key = uploadResult.r2Key; // Save this!
  ```
</Step>

<Step title="First Transcription">
  The initial transcription starts automatically after upload completion.
  
  ```javascript
  const jobId = uploadResult.jobId;
  // Monitor job and get results...
  ```
</Step>

<Step title="Re-transcribe with Different Settings">
  Use the saved `r2Key` to transcribe the same file with different parameters.
  
  ```javascript
  // Re-transcribe as SRT subtitle format
  const srtResult = await fetch('/v1/audio/transcriptions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      r2_bucket: 'sonartext-uploads',
      r2_key: r2Key,
      response_format: 'srt',
      enable_diarization: true
    })
  });
  ```
</Step>
</Steps>

## Best Practices

<AccordionGroup>
<Accordion title="🎯 Choose the Right Method">
  - **Direct upload**: Files < 150MB, immediate processing needed
  - **R2 reference**: Re-transcribing uploaded files with different settings
  - **S3 URL**: Integration with external storage systems
</Accordion>

<Accordion title="🔊 Optimize for Your Audio">
  - **Language**: Specify language for better accuracy (auto-detection is good but not perfect)
  - **Diarization**: Enable only if you need speaker identification (adds processing time)
  - **Format**: Choose format based on your use case (JSON for apps, SRT/VTT for video players)
</Accordion>

<Accordion title="⚡ Performance Tips">
  - **Word timestamps**: Only request if needed (increases response size)
  - **Streaming**: Responses stream in real-time for faster perceived performance
  - **Caching**: Cache results and use R2 re-transcription for different formats
</Accordion>

<Accordion title="💰 Cost Optimization">
  - **Re-use uploads**: Save `r2Key` from multipart uploads for multiple transcriptions
  - **Right-size diarization**: Use accurate speaker count ranges for better results
  - **Format selection**: Choose the most appropriate format to avoid unnecessary re-processing
</Accordion>
</AccordionGroup>

## Error Responses

<ResponseExample>
```json File Too Large (Direct Upload)
{
  "error": "file_too_large",
  "message": "File size exceeds maximum allowed for direct upload",
  "details": {
    "fileSize": 3221225472,
    "maxSize": 2147483648,
    "suggestion": "Use multipart upload for files larger than 2GB"
  }
}
```
</ResponseExample>

<ResponseExample>
```json R2 Object Not Found
{
  "error": "r2_object_not_found",
  "message": "Specified R2 object could not be found",
  "details": {
    "bucket": "sonartext-uploads",
    "key": "uploads/user123/nonexistent-file.wav"
  }
}
```
</ResponseExample>

<ResponseExample>
```json Insufficient Credits
{
  "error": "insufficient_credits",
  "message": "Insufficient credits to process this request",
  "balance": {
    "cents": 50,
    "dollars": 0.50,
    "formatted": "$0.50"
  },
  "cost": {
    "cents": 150,
    "dollars": 1.50,
    "formatted": "$1.50"
  },
  "estimatedDuration": 180
}
```
</ResponseExample>

<Card title="Monitor Jobs" icon="list-check" href="/api-reference/jobs/list">
  Learn how to track and manage your transcription jobs
</Card>
